{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have given 2000 document files which belongs to 20 classes.First we load the data set and removing all the stop words from the each file to get the pure word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = [\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\",\"the\" ,\"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\",\"i\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_feature=[] \n",
    "dict_features={} \n",
    "#Loading the data set\n",
    "ls=os.listdir('./20_newsgroups1')          \n",
    "for folders in ls:\n",
    "    files= os.listdir('./20_newsgroups1/'+folders) \n",
    "    word_feature.append(folders)\n",
    "    for file in files:\n",
    "        f=open('./20_newsgroups1/'+folders+'/'+file, 'r', encoding=\"ISO-8859-1\")\n",
    "        text=f.read()\n",
    "        tokens=re.compile('\\w+').findall(text)\n",
    "        words=[token.lower() for token in tokens if(token.isalpha() and len(token)>1 and token not in stop_words)]   \n",
    "        for word in words:\n",
    "            if word in dict_features:\n",
    "                dict_features[word]+=1\n",
    "            else:\n",
    "                    dict_features[word]=1  \n",
    "dict_features_copy=dict_features                    \n",
    "dict_features_copy=sorted(dict_features_copy.items(),key=lambda x:x[1],reverse=True) \n",
    "#we could have use hole dictionary for the feature but we have  lot of features that don't occurs in our data \n",
    "#using top 2000 words which have higher frequency.Lot of words have frequency 1,2,3 so we get rid of these words\n",
    "col=dict_features_copy[0:2000] \n",
    "col=dict(col) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature= []\n",
    "for key, value in col.items():\n",
    "    feature.append(key) \n",
    "#Creating empty dataframe with all features.we would be using words as feature\n",
    "df=pd.DataFrame(columns=feature)\n",
    "for folders in ls:\n",
    "    files= os.listdir('./20_newsgroups1/'+folders) \n",
    "#Going through all document built the dictionary     \n",
    "    for file in files:\n",
    "        f=open('./20_newsgroups1/'+folders+'/'+file, 'r', encoding=\"ISO-8859-1\")\n",
    "        text=f.read()\n",
    "        tokens=re.compile('\\w+').findall(text)\n",
    "        words=[token.lower() for token in tokens if(token.isalpha() and len(token)>1 and token not in stop_words)]\n",
    "        arr=np.zeros(2000)\n",
    "#For each document creating emprty numpy zero array with size of 2000\n",
    "        for word in words:\n",
    "            if word in feature:\n",
    "                arr[feature.index(word)]+=1\n",
    "        df1=pd.DataFrame([arr],columns=feature)        \n",
    "        df=df.append(df1,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('textclassification_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating target value Y  \n",
    "Y=[]\n",
    "for i in word_feature:\n",
    "    if i==\"soc.religion.christian\":\n",
    "        for j in range(997):\n",
    "            Y.append(i)\n",
    "    else:\n",
    "        for j in range(1000):\n",
    "            Y.append(i)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "target=pd.read_csv(\"textclassification_target.csv\",header=None)\n",
    "Y=target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training Naive Bayes (NB) classifier on training data.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spliting the data \n",
    "X_train,X_test,Y_train,Y_test=train_test_split(df,Y,test_size=0.20,random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15997, 2000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepesh/ENTER/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.85922360442582979"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#performance of multinomial naive bayes\n",
    "clf=MultinomialNB()\n",
    "clf.fit(X_train,Y_train)\n",
    "clf.score(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82825000000000004"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['comp.sys.mac.hardware', 'talk.politics.guns', 'comp.graphics', ...,\n",
       "       'misc.forsale', 'sci.space', 'misc.forsale'],\n",
       "      dtype='<U24')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred=clf.predict(X_test)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.81      0.81      0.81       215\n",
      "           comp.graphics       0.75      0.74      0.75       170\n",
      " comp.os.ms-windows.misc       0.96      0.12      0.22       205\n",
      "comp.sys.ibm.pc.hardware       0.61      0.84      0.70       195\n",
      "   comp.sys.mac.hardware       0.73      0.86      0.79       196\n",
      "          comp.windows.x       0.63      0.84      0.72       201\n",
      "            misc.forsale       0.80      0.93      0.86       192\n",
      "               rec.autos       0.85      0.90      0.88       202\n",
      "         rec.motorcycles       0.87      0.94      0.90       200\n",
      "      rec.sport.baseball       0.92      0.90      0.91       201\n",
      "        rec.sport.hockey       0.93      0.90      0.91       196\n",
      "               sci.crypt       0.97      0.93      0.95       193\n",
      "         sci.electronics       0.84      0.89      0.87       175\n",
      "                 sci.med       0.95      0.89      0.92       204\n",
      "               sci.space       0.93      0.92      0.93       203\n",
      "  soc.religion.christian       0.97      1.00      0.98       213\n",
      "      talk.politics.guns       0.87      0.89      0.88       227\n",
      "   talk.politics.mideast       0.95      0.91      0.93       198\n",
      "      talk.politics.misc       0.75      0.75      0.75       190\n",
      "      talk.religion.misc       0.72      0.63      0.68       224\n",
      "\n",
      "             avg / total       0.84      0.83      0.82      4000\n",
      "\n",
      "[[175   0   0   0   0   0   0   1   1   0   0   1   0   0   0   0   0   2\n",
      "    0  35]\n",
      " [  0 126   0   8  14  10   2   2   3   1   0   1   1   1   1   0   0   0\n",
      "    0   0]\n",
      " [  0  13  25  66  13  80   6   0   0   0   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   0 163  21   3   4   0   1   0   0   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   4   0  18 168   1   4   0   0   0   0   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0  14   1   5   5 169   1   0   2   1   0   0   1   1   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   3   0   0 178   9   0   0   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   7 182   6   0   0   0   5   0   1   0   1   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   1   4   6 189   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   1   0   2   2   3 181  11   0   0   0   1   0   0   0\n",
      "    0   0]\n",
      " [  1   0   0   0   0   0   2   2   2  11 176   0   1   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   2   0   1   1   2   1   0   0   0   0 179   4   1   1   0   1   0\n",
      "    0   0]\n",
      " [  0   1   0   4   6   0   4   2   1   0   0   0 156   0   0   0   0   0\n",
      "    1   0]\n",
      " [  0   1   0   0   0   0   1   3   5   1   0   0   8 182   3   0   0   0\n",
      "    0   0]\n",
      " [  1   3   0   0   0   0   2   1   0   0   0   0   3   3 186   0   1   0\n",
      "    3   0]\n",
      " [  0   0   0   0   0   1   0   0   0   0   0   0   0   0   0 212   0   0\n",
      "    0   0]\n",
      " [  0   0   0   1   0   0   1   1   5   0   0   1   0   0   0   0 202   0\n",
      "   10   6]\n",
      " [  0   0   0   0   1   0   3   1   0   0   1   0   0   1   0   0   1 180\n",
      "   10   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   1   3   0   2   3   1  18   6\n",
      "  142  13]\n",
      " [ 39   1   0   0   0   0   0   1   0   0   0   0   0   0   1   6   9   2\n",
      "   23 142]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report \n",
    "print(classification_report(Y_test, Y_pred))  \n",
    "print(confusion_matrix(Y_test,Y_pred)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
